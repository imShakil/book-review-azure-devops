# Deploy Book Review App (3 tier) Using Azure DevOps Pipeline

This project demonstrates deploying a 3-tier book review application to AWS using Infrastructure as Code (IaC) and CI/CD best practices via Azure DevOps.

## Architecture

- **Frontend**: React/Next.js application (EC2)
- **Backend**: Node.js API server (EC2)
- **Database**: AWS RDS
- **Infrastructure**: AWS VPC, Security Groups, EC2 instances

## Deployment Stack

- **Infrastructure**: Terraform for AWS resource provisioning
- **Configuration**: Ansible for application deployment and configuration
- **CI/CD**: Azure DevOps Pipeline for automated deployment

## Pipeline Stages

1. **terraformAction**: Provisions AWS infrastructure and generates Ansible inventory
2. **ansibleAction**: Configures applications on EC2 instances
3. **destroy**: Auto-destroys infrastructure if Ansible fails
4. **manualDestroy**: Manual approval-based destruction for successful deployments
5. **cleanup**: Cleans up agent resources

## Prerequisites

- Azure DevOps account
- AWS account with appropriate permissions
- Self-hosted Azure DevOps agent (aws-ubuntu pool)
- SSH key pair for EC2 access
- Terraform installed on agent
- Ansible installed on agent

## Variable Groups Setup

### aws-credentials

- `AWS_ACCESS_KEY_ID`: Your AWS access key
- `AWS_SECRET_ACCESS_KEY`: Your AWS secret key
- `AWS_DEFAULT_REGION`: AWS region (e.g., ap-southeast-1)

### terraform-vars

- `TF_PREFIX`: Resource prefix for naming
- `TF_RDS_NAME`: RDS database name
- `TF_RDS_ADMIN`: RDS admin username
- `TF_RDS_PASSWORD`: RDS admin password

## Secure Files

- `id_rsa.pub`: SSH public key for EC2 instances
- `id_rsa`: SSH private key for Ansible connections

## Pipeline Features

- **Conditional Destruction**: Automatically destroys infrastructure on Ansible failure
- **Manual Approval**: Requires approval before destroying successful deployments
- **Artifact Management**: Shares Terraform outputs with Ansible stage
- **SSH Key Management**: Handles SSH key generation and distribution
- **Error Handling**: Comprehensive cleanup on failures

## File Structure

```sh
book-review-azure-devops/
├── azure-pipelines.yml          # Main pipeline configuration
├── terraform/
│   ├── main.tf                  # Terraform infrastructure code
│   ├── variables.tf             # Terraform variables
│   └── outputs.tf               # Terraform outputs
├── ansible/
│   ├── bookreview.yaml          # Ansible playbook
│   ├── inventory.ini            # Generated by Terraform
│   └── group_vars/
│       └── rds_info.yml         # Generated RDS connection info
└── README.md
```

## Deployment Process

1. **Infrastructure Provisioning**
   - Creates AWS VPC with public/private subnets
   - Provisions RDS database in private subnet
   - Launches EC2 instances in public subnet
   - Generates SSH key pair for secure access

2. **Configuration Management**
   - Downloads generated inventory and RDS info
   - Configures EC2 instances using Ansible
   - Deploys frontend and backend applications
   - Sets up database connections

3. **Cleanup Strategy**
   - Auto-destroy on Ansible failure
   - Manual approval for successful deployments
   - Agent cleanup after pipeline completion

## Troubleshooting

### SSH Connection Issues

- Verify security group allows SSH (port 22)
- Check EC2 instances have public IPs
- Ensure SSH keys are properly configured

### Terraform Provider Errors

- Add `chmod +x .terraform/providers` after init
- Verify AWS credentials are correct
- Check Terraform state file permissions

### Ansible Connectivity

- Verify inventory.ini is generated correctly
- Check SSH private key permissions (600)
- Ensure instances are fully booted before Ansible runs

## Cost Optimization

- Pipeline uses approval gates to prevent unnecessary costs
- Automatic cleanup on failures
- Manual destroy requires approval to prevent accidental deletion
- No agent minutes consumed during approval wait

## Security Best Practices

- AWS credentials stored in Azure DevOps variable groups
- SSH keys managed as secure files
- RDS in private subnet with security group restrictions
- Terraform state managed securely
- No hardcoded secrets in pipeline files

## Step-by-Step Setup in Azure DevOps

### 1. Create Azure DevOps Project

1. Go to [Azure DevOps](https://dev.azure.com)
2. Click **New Project**
3. Enter project name: `book-review-deployment`
4. Set visibility to **Private**
5. Click **Create**

### 2. Import Repository

1. Go to **Repos** → **Files**
2. Click **Import a repository**
3. Enter this repository URL or upload your code
4. Click **Import**

### 3. Set Up Agent Pool

1. Go to **Project Settings** → **Agent pools**
2. Click **Add pool** → **New agent pool**
3. Name: `aws-ubuntu`
4. Install self-hosted agent on Ubuntu machine with Terraform and Ansible

### 4. Create Variable Groups

1. Go to **Pipelines** → **Library** → **Variable groups**
2. Click **+ Variable group**

**Create `aws-credentials` group:**

- Name: `aws-credentials`
- Add variables:
  - `AWS_ACCESS_KEY_ID`: Your AWS access key
  - `AWS_SECRET_ACCESS_KEY`: Your AWS secret key (click lock icon)
  - `AWS_DEFAULT_REGION`: `ap-southeast-1`

**Create `terraform-vars` group:**

- Name: `terraform-vars`
- Add variables:
  - `TF_PREFIX`: `bookreview`
  - `TF_RDS_NAME`: `bookdb`
  - `TF_RDS_ADMIN`: `admin`
  - `TF_RDS_PASSWORD`: `YourPassword123!` (click lock icon)

### 5. Upload SSH Keys

1. Generate SSH key pair: `ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa`
2. Go to **Pipelines** → **Library** → **Secure files**
3. Click **+ Secure file**
4. Upload `id_rsa.pub` (public key)
5. Upload `id_rsa` (private key)
6. Set permissions for both files to your pipeline

### 6. Create Environment

1. Go to **Pipelines** → **Environments**
2. Click **New environment**
3. Name: `production-destroy`
4. Click **Create**
5. Go to environment → **Approvals and checks**
6. Add **Approvals** → Select yourself as approver

### 7. Create Pipeline

1. Go to **Pipelines** → **Pipelines**
2. Click **New pipeline**
3. Select **Azure Repos Git**
4. Choose your repository
5. Select **Existing Azure Pipelines YAML file**
6. Path: `/azure-pipelines.yml`
7. Click **Continue** → **Run**

### 8. Monitor Pipeline

1. Pipeline will start automatically
2. Monitor each stage:
   - **terraformAction**: Creates AWS infrastructure
   - **ansibleAction**: Configures applications
   - **manualDestroy**: Waits for your approval to destroy
3. Approve destruction when ready in **Environments**

## Quick Start Summary

After completing the Azure DevOps setup:

1. Ensure your self-hosted agent has Terraform and Ansible installed
2. Verify AWS credentials have appropriate permissions
3. Run the pipeline and monitor each stage
4. Approve manual destroy when testing is complete
5. Check AWS console to verify resources are created/destroyed properly
